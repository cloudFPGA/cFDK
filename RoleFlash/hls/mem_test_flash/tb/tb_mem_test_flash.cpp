// *****************************************************************************
// *
// *                             cloudFPGA
// *            All rights reserved -- Property of IBM
// *
// *----------------------------------------------------------------------------
// *
// * Title : Test bench for the echo application in store-and-forward mode.
// *
// * File    : test_mem_test_flash_main.cpp
// *
// * Created : Apr. 2018
// * Authors : Francois Abel <fab@zurich.ibm.com>
// *
// * Devices : xcku060-ffva1156-2-i
// * Tools   : Vivado v2017.4 (64-bit)
// * Depends : None
// *
// * Description : The role application implements an echo loopback on the UDP
// *   and TCP connections. The application is said to be operating in
// *   "store-and-forward" mode because every received packet is first stored
// *   in the DDR4 before being read from that memory and being sent back.
// *
// * Infos & Remarks:
// *   The SHELL provides two logical memory ports for the ROLE to access the
// *   a physical channel of the DDR4. The interfaces of these two ports enable
// *   high throughput transfer of data between the AXI4 memory-mapped domain of
// *   the DDR4 and the AXI4-Stream domains of the ROLE. The interfaces are
// *   based on an underlying AXI-DataMover-IP from Xilinx which provides
// *   specific Memory-Map-to-Stream (MM2S) and a Stream-to-Memory-Map (S2MM)
// *   channels for handling the transaction between the two domains.
// *   For more details, refer to Xilinx/LogiCORE-IP-Product-Guide (PG022).
// *
// *   	 	+-------------------------------------------+
// *		|	ROLE (i.e. mem_test_flash.cpp)	|
// *		|                   						|
// *		|											|
// *	->--+ siUdp            [UDP]              soUdp +--->
// *		|											|
// *	->--+ siTcp            [TCP]              soTcp +--->
// *		|											|
// *    ->--+ siMemRdStsP0  [MEM-RD-P0]	   soMemRdCmdP0 +--->
// *    ->--+ siMemReadP0                               |
// *		|											|
// *    ->--+ siMemWrStsP0  [MEM-WR-P0]    soMemWrCmdP0 +--->
// *        |                              soMemWriteP0 +--->
// *        |											|
// *    ->--+ siMemRdStsP1 	[MEM-RD-P1]    soMemRdCmdP1 +--->
// *    ->--+ siMemReadP1                               |
// *		|											|
// *    ->--+ siMemWrStsP1  [MEM-WR-P1]    soMemWrCmdP1 +--->
// *        |                              soMemWriteP1 +--->
// *        |											|
// *   	 	+-------------------------------------------+
// *
// * Conventions:
// *   <pi> stands for "PortIn".
// *   <po> stands for "PortOut".
// *   <si> stands for "StreamIn".
// *   <so> stands for "StreamOut".
// *   <si><SRC>_<Itf1>_<Itf1.1>_<Itf1.1.1>_tdata stands for the "data" signals
// *        of an Axi4-Stream generated by the source (i.e. master) "SRC", out
// *        of its interface "Itf1" and its sub-interfaces "Itf1.1" and
// *        "Itf1.1.1".
// *
// *****************************************************************************
#include "../src/mem_test_flash.hpp"

using namespace std;
using namespace hls;

#define NR_UDP_PKT 		2	// The number of UDP packets to generate
#define NR_TCP_PKT 		2	// The number of TCP packets to generate
#define SIZEOF_UDP_PKT  4	// In number of 64-bit chunks
#define SIZEOF_TCP_PKT  4	// In number of 64-bit chunks

int main() {

	//-- SHELL / Role / Nts0 / Udp Interface
	stream<Axis<64> >		sSHL_Rol_Udp("sSHL_Rol_Udp");
	stream<Axis<64>	>		sROL_Shl_Udp("sROL_Shl_Udp");
	//-- SHELL / Role / Nts0 / Tcp Interface
	stream<Axis<64> >		sSHL_Rol_Tcp("sSHL_Rol_Tcp");
	stream<Axis<64>	>		sROL_Shl_Tcp("sROL_Shl_Tcp");
	//-- SHELL / Role / Mem / Mp0 Interface
	//---- Read Path (MM2S) ------------
	stream<DmCmd>			sROL_Shl_Mem_RdCmdP0("sROL_Shl_Mem_RdCmdP0");
	stream<DmSts>			sSHL_Rol_Mem_RdStsP0("sSHL_Rol_Mem_RdStsP0");
	stream<Axis<512> >  	sSHL_Rol_Mem_ReadP0("sSHL_Rol_Mem_ReadP0");
	//---- Write Path (S2MM) -----------
	stream<DmCmd>			sROL_Shl_Mem_WrCmdP0("sROL_Shl_Mem_WrCmdP0");
	stream<DmSts>			sSHL_Rol_Mem_WrStsP0("sSHL_Rol_Mem_WrStsP0");
	stream<Axis<512> >		sROL_Shl_Mem_WriteP0("sROL_Shl_Mem_WriteP0");
	//-- SHELL / Role / Mem / Mp1 Interface
	//---- Read Path (MM2S) ------------
    stream<DmCmd>			sROL_Shl_Mem_RdCmdP1("sROL_Shl_Mem_RdCmdP1");
	stream<DmSts>			sSHL_Rol_Mem_RdStsP1("sSHL_Rol_Mem_RdStsP1");
	stream<Axis<512> >		sSHL_Rol_Mem_ReadP1("sSHL_Rol_Mem_ReadP1");
    //---- Write Path (S2MM) -----------
	stream<DmCmd>			sROL_Shl_Mem_WrCmdP1("sROL_Shl_Mem_WrCmdP1");
	stream<DmSts>			sSHL_Rol_Mem_WrStsP1("sSHL_Rol_Mem_WrStsP1");
    stream<Axis<512> >  	sROL_Shl_Mem_WriteP1("sROL_Shl_Mem_WriteP1");

    Axis<64>				axis_SHL_Rol_Udp[NR_UDP_PKT*SIZEOF_UDP_PKT];
    Axis<64> 				axis_ROL_Shl_Udp[NR_UDP_PKT*SIZEOF_UDP_PKT];
    Axis<64>				axis_SHL_Rol_Tcp[NR_TCP_PKT*SIZEOF_TCP_PKT];
    Axis<64>				axis_ROL_Shl_Tcp[NR_TCP_PKT*SIZEOF_TCP_PKT];

    DmCmd					dmCmd_MemWrCmdP0, dmCmd_MemWrCmdP1;
    DmCmd					dmCmd_MemRdCmdP0, dmCmd_MemRdCmdP1;
    DmSts					dmSts_MemWrStsP0, dmSts_MemWrStsP1;
    DmSts					dmSts_MemRdStsP0, dmSts_MemRdStsP1;
    Axis<512>  				axis_MemP0[SIZEOF_UDP_PKT];
    Axis<512>				axis_MemP1[SIZEOF_TCP_PKT];

    int	udpWrCnt = 0;
    int tcpWrCnt = 0;
    int	udpRdCnt = 0;
    int tcpRdCnt = 0;
    int	memP0WrCnt = 0;
    int	memP1WrCnt = 0;
    int	memP0RdCnt = 0;
    int	memP1RdCnt = 0;
    bool memP0WrReq = false;
    bool memP1WrReq = false;
    bool memP0RdReq = false;
    bool memP1RdReq = false;

    // Prepare UDP and TCP input buffers
    for (int i=0; i < NR_UDP_PKT; i++) {
    	for (int j=0; j < SIZEOF_UDP_PKT; j++) {
    		//-- SHELL -> Role -> Nts0 -> Udp Interface
    		axis_SHL_Rol_Udp[i*SIZEOF_UDP_PKT+j].tdata = (j << 56) | (j << 48) | (j << 40) | (j << 32) |
    									   	   	   	     (j << 24) | (j << 16) | (j <<  8) | (j <<  0);
    		axis_SHL_Rol_Udp[i*SIZEOF_UDP_PKT+j].tkeep = 0xff;
    		axis_SHL_Rol_Udp[i*SIZEOF_UDP_PKT+j].tlast = ( j < SIZEOF_UDP_PKT-1) ? 0 : 1;
    	}
    }
    for (int i=0; i < NR_TCP_PKT; i++) {
    	for (int j=0; j < SIZEOF_TCP_PKT; j++) {
    		//-- SHELL -> Role -> Nts0 -> Tcp Interface
    		axis_SHL_Rol_Tcp[i*SIZEOF_TCP_PKT+j].tdata = (j << 56) | (j << 48) | (j << 40) | (j << 32) |
	       				 	 		 	   	   	   	     (j << 24) | (j << 16) | (j <<  8) | (j <<  0);
    		axis_SHL_Rol_Tcp[i*SIZEOF_TCP_PKT+j].tkeep = 0xff;
    		axis_SHL_Rol_Tcp[i*SIZEOF_TCP_PKT+j].tlast = ( j < SIZEOF_TCP_PKT-1) ? 0 : 1;
    	}
    }


    for (int loop=0; loop <3*(NR_UDP_PKT*SIZEOF_UDP_PKT+NR_TCP_PKT*SIZEOF_TCP_PKT); loop++) {

    	printf("**** LOOP=%2.2d ******************************\n", loop);

    	if (udpWrCnt < NR_UDP_PKT*SIZEOF_UDP_PKT) {
    		// Feed the UDP ROLE stream with data chunks
    		if (!sSHL_Rol_Udp.full()) {
    			printf("SHL->ROL->UDP : WrUdpChunck[%d]\n", udpWrCnt);
    			sSHL_Rol_Udp << axis_SHL_Rol_Udp[udpWrCnt++];
    		}
     	}
    	if (tcpWrCnt < NR_TCP_PKT*SIZEOF_TCP_PKT) {
    		// Feed the TCP ROLE stream with data chunks
    		if (!sSHL_Rol_Tcp.full()) {
    			printf("SHL->ROL->TCP : WrTcpChunck[%d]\n", tcpWrCnt);
    			sSHL_Rol_Tcp << axis_SHL_Rol_Tcp[tcpWrCnt++];
    		}
    	}

    	// Drain the UDP stream of the ROLE
    	if (!sROL_Shl_Udp.empty()) {
    		printf("ROL->SHL->UDP : RdUdpChunck[%d]\n", udpRdCnt);
    		sROL_Shl_Udp >> axis_ROL_Shl_Udp[udpRdCnt++];
    	}
    	// Drain TCP's ROLE stream
      	if (!sROL_Shl_Tcp.empty()) {
      		printf("ROL->SHL->TCP : RdTcpChunck[%d]\n", tcpRdCnt);
      		sROL_Shl_Tcp >> axis_ROL_Shl_Tcp[tcpRdCnt++];
      	}


    	//-------------------------------------------------
    	//-- DUT
    	//-------------------------------------------------
    	mem_test_flash_main(
			//-- SHELL / Role / Nts0 / Udp Interface
			sSHL_Rol_Udp, sROL_Shl_Udp,
			//-- SHELL / Role / Nts0 / Tcp Interface
			sSHL_Rol_Tcp, sROL_Shl_Tcp,
			//-- SHELL / Role / Mem / Mp0 Interface
			sROL_Shl_Mem_RdCmdP0, sSHL_Rol_Mem_RdStsP0, sSHL_Rol_Mem_ReadP0,
			sROL_Shl_Mem_WrCmdP0, sSHL_Rol_Mem_WrStsP0, sROL_Shl_Mem_WriteP0,
			//-- SHELL / Role / Mem / Mp1 Interface
			sROL_Shl_Mem_RdCmdP1, sSHL_Rol_Mem_RdStsP1, sSHL_Rol_Mem_ReadP1,
			sROL_Shl_Mem_WrCmdP1, sSHL_Rol_Mem_WrStsP1, sROL_Shl_Mem_WriteP1
	    );


    	//-------------------------------------------------
    	// Handle the Memory Write Commands for P0 & P1
    	//-------------------------------------------------
    	if (!memP0WrReq && !sROL_Shl_Mem_WrCmdP0.empty()) {
    		sROL_Shl_Mem_WrCmdP0 >> dmCmd_MemWrCmdP0;
    		printf("ROL->SHL->MEM->CMDP0 : Received WrCmd (SADDR=0x%8.8x, BTT=%d). \n", dmCmd_MemWrCmdP0.saddr.to_uint(), dmCmd_MemWrCmdP0.bbt.to_uint());
    		memP0WrReq = true;
    	}
    	//--
    	if (!memP1WrReq && !sROL_Shl_Mem_WrCmdP1.empty()) {
    		sROL_Shl_Mem_WrCmdP1 >> dmCmd_MemWrCmdP1;
    		printf("ROL->SHL->MEM->CMDP1 : Received WrCmd (SADDR=0x%8.8x, BTT=%d). \n", dmCmd_MemWrCmdP1.saddr.to_uint(), dmCmd_MemWrCmdP1.bbt.to_uint());
    		memP1WrReq = true;
    	}

    	//-------------------------------------------------
    	// Handle the Memory Write Bursts for P0 & P1
    	//-------------------------------------------------
    	if (memP0WrReq && !sROL_Shl_Mem_WriteP0.empty()) {
    		sROL_Shl_Mem_WriteP0 >> axis_MemP0[memP0WrCnt];
    		if (axis_MemP0[memP0WrCnt].tlast) {
    			printf("ROL->SHL->MEM->WRITEP0 : Received a burst of %d data from the ROLE (.i.e. %d bytes).\n", memP0WrCnt+1, (memP0WrCnt+1)*8);
    			memP0WrCnt = 0;
    			memP0WrReq = false;
    			// Send Status byte for P0
    			if (!sSHL_Rol_Mem_WrStsP0.full()) {
    				sSHL_Rol_Mem_WrStsP0 << dmSts_MemWrStsP0;
    				printf("MEM->SHL->ROL->STSP0 : Sent WrSts byte to the ROLE. \n");
    			}
    			else {
    				printf(">>>> ERROR: Cannot send memory write P0 status back to the ROLE (stream is full).\n");
    				return(1);
    			}
    		}
    		else
    			memP0WrCnt++;
    	}
   		//--
   		if (memP1WrReq && !sROL_Shl_Mem_WriteP1.empty()) {
   			sROL_Shl_Mem_WriteP1 >> axis_MemP1[memP1WrCnt];
   			if (axis_MemP1[memP1WrCnt].tlast) {
   				printf("ROL->SHL->MEM->WRITEP1 : Received a burst of %d data from the ROLE (.i.e. %d bytes).\n", memP1WrCnt+1, (memP1WrCnt+1)*8);
   				memP1WrCnt = 0;
   				memP1WrReq = false;
   				// Send Status byte for P1
   				if (!sSHL_Rol_Mem_WrStsP1.full()) {
   					sSHL_Rol_Mem_WrStsP1 << dmSts_MemWrStsP1;
   					printf("MEM->SHL->ROL->STSP1 : Sent WrSts byte to the ROLE. \n");
   				}
   				else {
   					printf(">>>> ERROR: Cannot send memory write P1 status back to the ROLE (stream is full).\n");
   					return(1);
   				}
   			}
   			else
   				memP1WrCnt++;
   		}

    	//-------------------------------------------------
    	// Handle the Memory Read Commands for P0 & P1
    	//-------------------------------------------------
    	if (!memP0RdReq && !sROL_Shl_Mem_RdCmdP0.empty()) {
	    	sROL_Shl_Mem_RdCmdP0 >> dmCmd_MemRdCmdP0;
	    	printf("ROL->SHL->MEM->CMDP0 : Received RdCmd (SADDR=0x%8.8x, BTT=%d). \n", dmCmd_MemRdCmdP0.saddr.to_uint(), dmCmd_MemRdCmdP0.bbt.to_uint());
	    	memP0RdReq = true;
    	}
    	 if (!memP1RdReq && !sROL_Shl_Mem_RdCmdP1.empty()) {
    		sROL_Shl_Mem_RdCmdP1 >> dmCmd_MemRdCmdP1;
    		printf("ROL->SHL->MEM->CMDP1 : Received RdCmd (SADDR=0x%8.8x, BTT=%d). \n", dmCmd_MemRdCmdP1.saddr.to_uint(), dmCmd_MemRdCmdP1.bbt.to_uint());
    		memP1RdReq = true;
    	 }

    	 //-------------------------------------------------
    	 // Handle the Memory Read Bursts for P0 & P1
    	 //-------------------------------------------------
    	 if (memP0RdReq && !sSHL_Rol_Mem_ReadP0.full()) {
    		sSHL_Rol_Mem_ReadP0 << axis_MemP0[memP0RdCnt];
    		if (axis_MemP0[memP0RdCnt].tlast) {
    			printf("SHL->ROL->MEM->READP0 : Sent a burst of %d data (.i.e. %d bytes).\n", memP0RdCnt+1, (memP0RdCnt+1)*8);
    			memP0RdCnt = 0;
    			memP0RdReq = false;
	    		// Send Status byte for P0
	    		if (!sSHL_Rol_Mem_RdStsP0.full()) {
	    			sSHL_Rol_Mem_RdStsP0 << dmSts_MemRdStsP0;
	    			printf("MEM->SHL->ROL->STSP0 : Sent RdSts byte to the ROLE. \n");
	    		}
	    		else {
	    			printf(">>>> ERROR: Cannot send memory read P0 status back to the ROLE (stream is full).\n");
	    			return(1);
	    		}
	    	}
    		else
    			memP0RdCnt++;
    	 }
    	 if (memP1RdReq && !sSHL_Rol_Mem_ReadP1.full()) {
    		sSHL_Rol_Mem_ReadP1 << axis_MemP1[memP1RdCnt];
    		if (axis_MemP1[memP1RdCnt].tlast) {
    			printf("SHL->ROL->MEM->READP1 : Sent a burst of %d data (.i.e. %d bytes).\n", memP1RdCnt+1, (memP1RdCnt+1)*8);
    			memP1RdCnt = 0;
    			memP1RdReq = false;
    			// Send Status byte for P1
    			if (!sSHL_Rol_Mem_RdStsP1.full()) {
    				sSHL_Rol_Mem_RdStsP1 << dmSts_MemRdStsP1;
    				printf("MEM->SHL->ROL->STSP1 : Sent RdSts byte to the ROLE. \n");
    			}
    			else {
    				printf(">>>> ERROR: Cannot send memory read P1 status back to the ROLE (stream is full).\n");
    				return(1);
    			}
    		}
    		else
    			memP1RdCnt++;
    	 }

    }  // End: while()

    int udpErrs = 0;
    //-----------------------------------------------------
    // Final Test - Compare UDP/Tx and UDP/Rx chunks
    //-----------------------------------------------------
    for (int i=0; i<NR_UDP_PKT*SIZEOF_UDP_PKT; i++) {
    	if (axis_SHL_Rol_Udp[i].tdata != axis_ROL_Shl_Udp[i].tdata) {
    		udpErrs++;
    		printf(">>>> ERROR: UDP/Rx = 0x%8.8X - Expected 0x%8.8X. \n", axis_ROL_Shl_Udp[i].tdata.to_uint64(), axis_SHL_Rol_Udp[i].tdata.to_uint64());
    	}
    }
    int tcpErrs = 0;
    //-----------------------------------------------------
    // Final Test - Compare TCP/Tx and TCP/Rx chunks
    //-----------------------------------------------------
    for (int i=0; i<NR_TCP_PKT*SIZEOF_TCP_PKT; i++) {
    	if (axis_SHL_Rol_Tcp[i].tdata != axis_ROL_Shl_Tcp[i].tdata) {
    		tcpErrs++;
    		printf(">>>> ERROR: TCP/Rx = 0x%8.8X - Expected 0x%8.8X. \n", axis_ROL_Shl_Tcp[i].tdata.to_uint64(), axis_SHL_Rol_Tcp[i].tdata.to_uint64());
    	}
    }


    printf("#####################################################\n");
    if (udpErrs | tcpErrs)
    	printf("## ERROR - TESTBENCH FAILED (RC=%d) !!!             ##\n", udpErrs+tcpErrs);
   	else
   		printf("## SUCCESSFULL END OF TESTBENCH (RC=0)             ##\n");

   	 printf("#####################################################\n");
   	return(udpErrs+tcpErrs);

}
